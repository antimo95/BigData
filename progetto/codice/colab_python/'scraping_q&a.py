# -*- coding: utf-8 -*-
"""'Scraping_Q&A.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EMFdc36odU_5f05hIvOCjA1CZwNj-PZX

# Definizione di Web Scraping

*Il web scraping è il processo di estrazione dei dati dai siti web. Può essere effettuato manualmente, ma più spesso viene automatizzato utilizzando strumenti software.*
"""

!pip install selenium

from google.colab import drive
drive.mount('/content/drive')

!apt-get update
!apt-get install -y chromium-browser chromium-chromedriver

# Commented out IPython magic to ensure Python compatibility.
# %ls /usr/bin/

from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

def web_driver():
    options = webdriver.ChromeOptions()

    options.add_argument("--verbose")
    options.add_argument('--no-sandbox')
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument("--window-size=1920,1200")
    options.add_argument('--disable-dev-shm-usage')


    # Initialize the WebDriver instance with the specified options
    # ChromeDriver è a conti fatti il software installato che ci permetterà di pilotare il nostro browser
    # Driver fa uso di tale software ed è utilizzato per impartire degli ordini al browser
    driver = webdriver.Chrome(options=options)
    return driver

def pulisci_stringa_con_vettore(stringa, parole_chiave):
    # Trova l'indice della prima parola chiave trovata nella stringa
    min_indice = len(stringa)
    for parola_chiave in parole_chiave:
        indice = stringa.find(parola_chiave)
        if indice != -1 and indice < min_indice:
            min_indice = indice
    # Se una parola chiave è trovata, ritorna la parte della stringa prima della parola chiave
    if min_indice != len(stringa):
        return stringa[:min_indice]
    # Se nessuna parola chiave è trovata, ritorna la stringa originale
    return stringa

"""#Scraping mediante SELENIUM dal sito Dica33

## Malattie cuore/sangue
"""

from datetime import datetime

def time_converter(data_testuale):
    """
    Converts a date string in Italian format to ISO 8601 format.

    Args:
        data_testuale (str): The date string in Italian format (e.g., "26 ottobre 2024").

    Returns:
        str: The date in ISO 8601 format (YYYY-MM-DD).
    """
    # Define a dictionary to map Italian month names to English month names
    italian_to_english_months = {
        "gennaio": "January",
        "febbraio": "February",
        "marzo": "March",
        "aprile": "April",
        "maggio": "May",
        "giugno": "June",
        "luglio": "July",
        "agosto": "August",
        "settembre": "September",
        "ottobre": "October",
        "novembre": "November",
        "dicembre": "December",
    }

    # Replace the Italian month name with the English month name
    for italian_month, english_month in italian_to_english_months.items():
        data_testuale = data_testuale.replace(italian_month, english_month)

    # Converti la data in un oggetto datetime
    data_datetime = datetime.strptime(data_testuale, "%d %B %Y")

    # Rimuovi l'ora e mantieni solo la data
    data_solo_data = data_datetime.date()

    # Converti la data in un formato ISO 8601 (senza ora)
    data_iso = data_solo_data.isoformat()
    return data_iso

# Esempio di utilizzo
data_testuale = "26 ottobre 2024"
data_iso = time_converter(data_testuale)
print(data_iso)
# Output: 2024-10-26

import pandas as pd

# 1. Initialization and Opening the Target URL:
driver = web_driver()
driver.get('https://www.dica33.it/esperto-risponde/domande-cuore-circolazione-e-malattie-del-sangue/')
parole_chiave = ["www.", "http://", "https://", "Dr." ,"Dott.", "Dott.ssa", "Cordiali", "Saluti", "Cordialmente", "Prego", "Arrivederci", "Prof."]
data = []
# 2. Loop to Navigate Through Pages:
for i in range(50):
    # 3. Extracting Question URLs:
    for urls in driver.find_elements(By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[14]/div/div/h3/a'):
        url = urls.get_attribute("href")
        print("URL QA: {}".format(url))
        print()

        curr_driver = web_driver()
        curr_driver.get(url)

        # 4. Waiting for Elements to Load:
        WebDriverWait(curr_driver, 20).until(
            EC.presence_of_element_located((By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[15]'))
        )

        # 5. Extracting and Storing Data:
        try:
            date = curr_driver.find_element(By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[15]/div[3]').text
            d = time_converter(date)
        except:
            d = None
            date = None

        try:
            question = curr_driver.find_element(By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[15]').text.split('\n')[5]
        except:
            question = None

        try:
            answer = curr_driver.find_element(By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[15]/div[4]/div').text
        except:
            answer = None

        print("Question Date: {}".format(d))
        print()
        print("Question: {}".format(question))
        print()
        print("Answer: {}".format(answer))
        print()

        data.append({
            "URL": url,
            "Question Date": d,
            "Question": question,
            "Answer": answer
        })

        # 6. Closing WebDriver Instances:
        curr_driver.quit()

    # 7. Navigating to the Next Page:
    next_page_script = "document.getElementById('pgg').value='{}'; paginazione.submit();".format(i + 2)
    driver.execute_script(next_page_script)

    print("Navigating to Next Page")
    print("-----------------------")
    print(i)

# 6. Closing WebDriver Instances:
driver.quit()

# Create DataFrame
df = pd.DataFrame(data)

df.head(10)

# 1. Importing the re Module:
import re

new_data = []

# 2. Iterating over Rows of the DataFrame df:
for i, row in df.iterrows():
    raw_answer = row['Answer']

    # 3. Extracting Information using Regular Expressions:
    match_prof = re.search(r'Prof\.\s+((?:\w+\s?)+)', raw_answer)
    match_doctor = re.search(r'Dott\.\s+((?:\w+\s?)+)', raw_answer)

    # 4. Determining the Type of Professional and Name:
    signature = ""
    if match_doctor:
        doctor = match_doctor.group(1).strip()
        signature = "Dott"
    elif match_prof:
        doctor = match_prof.group(1).strip()
        signature = "Prof"
    else:
        doctor = None

    # 5. Extracting the Response Date:
    match_date = re.search(r'Risposta del (\d+ \w+ \d+)', raw_answer)


    # 6. Extracting Additional Information:
    match_job = re.search(r'{}\. \w+ \w+\n(.+)'.format(signature), raw_answer)
    match_specialization = re.search(r'Specialista in (.+)', raw_answer)
    match_location = re.search(r'(\w+ \(\w+\))$', raw_answer)

    # 7. Assigning Results of Regex Searches:
    if match_date:
        d = match_date.group(1).strip()
    else:
        date = None

    if match_job:
        job = match_job.group(1).strip()
    else:
        job = None

    if match_specialization:
        specialization = match_specialization.group(1).strip()
    else:
        specialization = None

    if match_location:
        location = match_location.group(1)
    else:
        location = None

    # 8. Extracting the Main Response:
    if location is not None:
        match_answer = re.search(r'Risposta a cura di:\n{}\..+?\n\n(.+?)\n{}\.'.format(signature, signature), raw_answer, re.DOTALL)
        if match_answer:
            answer = match_answer.group(1).replace('\n', ' ').strip()
        else:
            answer = None
    else:
        if signature == "Prof":
            start_index = match_prof.end()
            answer = raw_answer[start_index:].strip()
        elif signature == "Dott":
            start_index = match_doctor.end()
            answer = raw_answer[start_index:].strip()
        else:
            answer = None

    date_conv = time_converter(date)
    new_data.append({
            "URL": row["URL"],
            "Question Date": row["Question Date"],
            "Question": row["Question"],
            "Answer Date": date_conv,
            "Answer": answer,
            "Doctor": doctor,
            "Job": job,
            "Specialization": specialization,
            "Location": location
        })

# Create DataFrame
new_df = pd.DataFrame(new_data)

new_df

new_df.to_csv("/content/drive/MyDrive/dica33_dataset_cuore.csv", index=False)

"""## Apparato respiratorio"""

import pandas as pd

# 1. Initialization and Opening the Target URL:
driver = web_driver()
driver.get('https://www.dica33.it/esperto-risponde/domande-apparato-respiratorio/')
parole_chiave = ["www.", "http://", "https://", "Dr." ,"Dott.", "Dott.ssa", "Cordiali", "Saluti", "Cordialmente", "Prego", "Arrivederci", "Prof."]
data = []
# 2. Loop to Navigate Through Pages:
for i in range(50):
    # 3. Extracting Question URLs:
    for urls in driver.find_elements(By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[14]/div/div/h3/a'):
        url = urls.get_attribute("href")
        print("URL QA: {}".format(url))
        print()

        curr_driver = web_driver()
        curr_driver.get(url)

        # 4. Waiting for Elements to Load:
        WebDriverWait(curr_driver, 20).until(
            EC.presence_of_element_located((By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[15]'))
        )

        # 5. Extracting and Storing Data:
        try:
            date = curr_driver.find_element(By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[15]/div[3]').text
            d = time_converter(date)
        except:
            d = None
            date = None

        try:
            question = curr_driver.find_element(By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[15]').text.split('\n')[5]
        except:
            question = None

        try:
            answer = curr_driver.find_element(By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[15]/div[4]/div').text
        except:
            answer = None

        print("Question Date: {}".format(d))
        print()
        print("Question: {}".format(question))
        print()
        print("Answer: {}".format(answer))
        print()

        data.append({
            "URL": url,
            "Question Date": d,
            "Question": question,
            "Answer": answer
        })

        # 6. Closing WebDriver Instances:
        curr_driver.quit()

    # 7. Navigating to the Next Page:
    next_page_script = "document.getElementById('pgg').value='{}'; paginazione.submit();".format(i + 2)
    driver.execute_script(next_page_script)

    print("Navigating to Next Page")
    print("-----------------------")
    print()

# 6. Closing WebDriver Instances:
driver.quit()

# Create DataFrame
df = pd.DataFrame(data)

# 1. Importing the re Module:
import re

new_data = []

# 2. Iterating over Rows of the DataFrame df:
for i, row in df.iterrows():
    raw_answer = row['Answer']

    # 3. Extracting Information using Regular Expressions:
    match_prof = re.search(r'Prof\.\s+((?:\w+\s?)+)', raw_answer)
    match_doctor = re.search(r'Dott\.\s+((?:\w+\s?)+)', raw_answer)

    # 4. Determining the Type of Professional and Name:
    signature = ""
    if match_doctor:
        doctor = match_doctor.group(1).strip()
        signature = "Dott"
    elif match_prof:
        doctor = match_prof.group(1).strip()
        signature = "Prof"
    else:
        doctor = None

    # 5. Extracting the Response Date:
    match_date = re.search(r'Risposta del (\d+ \w+ \d+)', raw_answer)


    # 6. Extracting Additional Information:
    match_job = re.search(r'{}\. \w+ \w+\n(.+)'.format(signature), raw_answer)
    match_specialization = re.search(r'Specialista in (.+)', raw_answer)
    match_location = re.search(r'(\w+ \(\w+\))$', raw_answer)

    # 7. Assigning Results of Regex Searches:
    if match_date:
        d = match_date.group(1).strip()
    else:
        date = None

    if match_job:
        job = match_job.group(1).strip()
    else:
        job = None

    if match_specialization:
        specialization = match_specialization.group(1).strip()
    else:
        specialization = None

    if match_location:
        location = match_location.group(1)
    else:
        location = None

    # 8. Extracting the Main Response:
    if location is not None:
        match_answer = re.search(r'Risposta a cura di:\n{}\..+?\n\n(.+?)\n{}\.'.format(signature, signature), raw_answer, re.DOTALL)
        if match_answer:
            answer = match_answer.group(1).replace('\n', ' ').strip()
        else:
            answer = None
    else:
        if signature == "Prof":
            start_index = match_prof.end()
            answer = raw_answer[start_index:].strip()
        elif signature == "Dott":
            start_index = match_doctor.end()
            answer = raw_answer[start_index:].strip()
        else:
            answer = None

    date_conv = time_converter(date)
    new_data.append({
            "URL": row["URL"],
            "Question Date": row["Question Date"],
            "Question": row["Question"],
            "Answer Date": date_conv,
            "Answer": answer,
            "Doctor": doctor,
            "Job": job,
            "Specialization": specialization,
            "Location": location
        })

# Create DataFrame
new_df = pd.DataFrame(new_data)

new_df.head(5)

new_df.to_csv("/content/drive/MyDrive/dica33_dataset_apparato_respiratorio.csv", index=False)

"""##Domande legate al Fegato"""

import pandas as pd

# 1. Initialization and Opening the Target URL:
driver = web_driver()
driver.get('https://www.dica33.it/esperto-risponde/domande-fegato/')
parole_chiave = ["www.", "http://", "https://", "Dr." ,"Dott.", "Dott.ssa", "Cordiali", "Saluti", "Cordialmente", "Prego", "Arrivederci", "Prof."]
data = []
# 2. Loop to Navigate Through Pages:
for i in range(20):
    # 3. Extracting Question URLs:
    for urls in driver.find_elements(By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[14]/div/div/h3/a'):
        url = urls.get_attribute("href")
        print("URL QA: {}".format(url))
        print()

        curr_driver = web_driver()
        curr_driver.get(url)

        # 4. Waiting for Elements to Load:
        WebDriverWait(curr_driver, 20).until(
            EC.presence_of_element_located((By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[15]'))
        )

        # 5. Extracting and Storing Data:
        try:
            date = curr_driver.find_element(By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[15]/div[3]').text
            d = time_converter(date)
        except:
            d = None
            date = None

        try:
            question = curr_driver.find_element(By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[15]').text.split('\n')[5]
        except:
            question = None

        try:
            answer = curr_driver.find_element(By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[15]/div[4]/div').text
        except:
            answer = None

        print("Question Date: {}".format(d))
        print()
        print("Question: {}".format(question))
        print()
        print("Answer: {}".format(answer))
        print()

        data.append({
            "URL": url,
            "Question Date": d,
            "Question": question,
            "Answer": answer
        })

        # 6. Closing WebDriver Instances:
        curr_driver.quit()

    # 7. Navigating to the Next Page:
    next_page_script = "document.getElementById('pgg').value='{}'; paginazione.submit();".format(i + 2)
    driver.execute_script(next_page_script)

    print("Navigating to Next Page")
    print("-----------------------")
    print()

# 6. Closing WebDriver Instances:
driver.quit()

# Create DataFrame
df = pd.DataFrame(data)

# 1. Importing the re Module:
import re

new_data = []

# 2. Iterating over Rows of the DataFrame df:
for i, row in df.iterrows():
    raw_answer = row['Answer']

    # 3. Extracting Information using Regular Expressions:
    match_prof = re.search(r'Prof\.\s+((?:\w+\s?)+)', raw_answer)
    match_doctor = re.search(r'Dott\.\s+((?:\w+\s?)+)', raw_answer)

    # 4. Determining the Type of Professional and Name:
    signature = ""
    if match_doctor:
        doctor = match_doctor.group(1).strip()
        signature = "Dott"
    elif match_prof:
        doctor = match_prof.group(1).strip()
        signature = "Prof"
    else:
        doctor = None

    # 5. Extracting the Response Date:
    match_date = re.search(r'Risposta del (\d+ \w+ \d+)', raw_answer)


    # 6. Extracting Additional Information:
    match_job = re.search(r'{}\. \w+ \w+\n(.+)'.format(signature), raw_answer)
    match_specialization = re.search(r'Specialista in (.+)', raw_answer)
    match_location = re.search(r'(\w+ \(\w+\))$', raw_answer)

    # 7. Assigning Results of Regex Searches:
    if match_date:
        d = match_date.group(1).strip()
    else:
        date = None

    if match_job:
        job = match_job.group(1).strip()
    else:
        job = None

    if match_specialization:
        specialization = match_specialization.group(1).strip()
    else:
        specialization = None

    if match_location:
        location = match_location.group(1)
    else:
        location = None

    # 8. Extracting the Main Response:
    if location is not None:
        match_answer = re.search(r'Risposta a cura di:\n{}\..+?\n\n(.+?)\n{}\.'.format(signature, signature), raw_answer, re.DOTALL)
        if match_answer:
            answer = match_answer.group(1).replace('\n', ' ').strip()
        else:
            answer = None
    else:
        if signature == "Prof":
            start_index = match_prof.end()
            answer = raw_answer[start_index:].strip()
        elif signature == "Dott":
            start_index = match_doctor.end()
            answer = raw_answer[start_index:].strip()
        else:
            answer = None

    date_conv = time_converter(date)
    new_data.append({
            "URL": row["URL"],
            "Question Date": row["Question Date"],
            "Question": row["Question"],
            "Answer Date": date_conv,
            "Answer": answer,
            "Doctor": doctor,
            "Job": job,
            "Specialization": specialization,
            "Location": location
        })

# Create DataFrame
new_df = pd.DataFrame(new_data)

new_df.head(5)

new_df.to_csv("/content/drive/MyDrive/dica33_dataset_fegato.csv", index=False)

"""## Alimentazione / Naso&MalDiGola / Mente&Cervello"""

import pandas as pd

#https://www.dica33.it/esperto-risponde/domande-alimentazione//
#https://www.dica33.it/esperto-risponde/domande-orecchie-naso-e-gola//
#https://www.dica33.it/esperto-risponde/domande-mente-e-cervello

# 1. Initialization and Opening the Target URL:
driver = web_driver()
driver.get('https://www.dica33.it/esperto-risponde/domande-mente-e-cervello')
parole_chiave = ["www.", "http://", "https://", "Dr." ,"Dott.", "Dott.ssa", "Cordiali", "Saluti", "Cordialmente", "Prego", "Arrivederci", "Prof."]
data = []
# 2. Loop to Navigate Through Pages:
for i in range(100):
    # 3. Extracting Question URLs:
    for urls in driver.find_elements(By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[14]/div/div/h3/a'):
        url = urls.get_attribute("href")
        print("URL QA: {}".format(url))
        print()

        curr_driver = web_driver()
        curr_driver.get(url)

        # 4. Waiting for Elements to Load:
        WebDriverWait(curr_driver, 20).until(
            EC.presence_of_element_located((By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[15]'))
        )

        # 5. Extracting and Storing Data:
        try:
            date = curr_driver.find_element(By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[15]/div[3]').text
            d = time_converter(date)
        except:
            d = None
            date = None

        try:
            question = curr_driver.find_element(By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[15]').text.split('\n')[5]
        except:
            question = None

        try:
            answer = curr_driver.find_element(By.XPATH, '//*[@id="wrap"]/div/div/div/div[2]/div[1]/main/article/div[15]/div[4]/div').text
        except:
            answer = None

        print("Question Date: {}".format(d))
        print()
        print("Question: {}".format(question))
        print()
        print("Answer: {}".format(answer))
        print()

        data.append({
            "URL": url,
            "Question Date": d,
            "Question": question,
            "Answer": answer
        })

        # 6. Closing WebDriver Instances:
        curr_driver.quit()

    # 7. Navigating to the Next Page:
    next_page_script = "document.getElementById('pgg').value='{}'; paginazione.submit();".format(i + 2)
    driver.execute_script(next_page_script)

    print("Navigating to Next Page")
    print("-----------------------")
    print(i)

# 6. Closing WebDriver Instances:
driver.quit()

# Create DataFrame
df = pd.DataFrame(data)

# 1. Importing the re Module:
import re

new_data = []

# 2. Iterating over Rows of the DataFrame df:
for i, row in df.iterrows():
    raw_answer = row['Answer']

    # 3. Extracting Information using Regular Expressions:
    match_prof = re.search(r'Prof\.\s+((?:\w+\s?)+)', raw_answer)
    match_doctor = re.search(r'Dott\.\s+((?:\w+\s?)+)', raw_answer)

    # 4. Determining the Type of Professional and Name:
    signature = ""
    if match_doctor:
        doctor = match_doctor.group(1).strip()
        signature = "Dott"
    elif match_prof:
        doctor = match_prof.group(1).strip()
        signature = "Prof"
    else:
        doctor = None

    # 5. Extracting the Response Date:
    match_date = re.search(r'Risposta del (\d+ \w+ \d+)', raw_answer)


    # 6. Extracting Additional Information:
    match_job = re.search(r'{}\. \w+ \w+\n(.+)'.format(signature), raw_answer)
    match_specialization = re.search(r'Specialista in (.+)', raw_answer)
    match_location = re.search(r'(\w+ \(\w+\))$', raw_answer)

    # 7. Assigning Results of Regex Searches:
    if match_date:
        d = match_date.group(1).strip()
    else:
        date = None

    if match_job:
        job = match_job.group(1).strip()
    else:
        job = None

    if match_specialization:
        specialization = match_specialization.group(1).strip()
    else:
        specialization = None

    if match_location:
        location = match_location.group(1)
    else:
        location = None

    # 8. Extracting the Main Response:
    if location is not None:
        match_answer = re.search(r'Risposta a cura di:\n{}\..+?\n\n(.+?)\n{}\.'.format(signature, signature), raw_answer, re.DOTALL)
        if match_answer:
            answer = match_answer.group(1).replace('\n', ' ').strip()
        else:
            answer = None
    else:
        if signature == "Prof":
            start_index = match_prof.end()
            answer = raw_answer[start_index:].strip()
        elif signature == "Dott":
            start_index = match_doctor.end()
            answer = raw_answer[start_index:].strip()
        else:
            answer = None

    date_conv = time_converter(date)
    new_data.append({
            "URL": row["URL"],
            "Question Date": row["Question Date"],
            "Question": row["Question"],
            "Answer Date": date_conv,
            "Answer": answer,
            "Doctor": doctor,
            "Job": job,
            "Specialization": specialization,
            "Location": location
        })

# Create DataFrame
new_df = pd.DataFrame(new_data)

new_df

#new_df.to_csv("/content/drive/MyDrive/dica33_dataset_alimentazione.csv", index=False)
#new_df.to_csv("/content/drive/MyDrive/dica33_dataset_naso.csv", index=False)
new_df.to_csv("/content/drive/MyDrive/dica33_dataset_mente.csv", index=False)

"""#Scraping sito MioDottore

## Domande legate all'ansia

Nota sull'xpath: /html/body/div[3]/div[2]/main/div[2]/section[3]/div[2]/div[1]/div[1]/div

L'XPath che ottieni dall'esploratore risorse del browser è un XPath assoluto, che rappresenta il percorso completo dall'elemento radice (<html>) fino all'elemento desiderato. Questo tipo di XPath è molto specifico e può diventare fragile se la struttura della pagina cambia.
Per rendere il tuo XPath più robusto, è consigliabile utilizzare un XPath relativo che si basa su attributi unici degli elementi. Ad esempio, se l'elemento che stai cercando ha un attributo class o id unico, puoi usarlo per creare un XPath più affidabile:

//div[@class='h4 font-weight-normal doctor-question-body mb-2']/a[@class='text-body']

Nota sul ''main''
In HTML, <main> è un elemento semantico che rappresenta il contenuto principale del documento. Questo elemento è utilizzato per racchiudere il contenuto principale della pagina, escludendo elementi come intestazioni, barre laterali, piè di pagina e altri contenuti ripetitivi. L'uso di <main> aiuta i motori di ricerca e gli screen reader a comprendere meglio la struttura della pagina e a identificare rapidamente il contenuto principale.
"""

import pandas as pd
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# 1. Inizializzazione e Apertura dell'URL di Destinazione:
driver = web_driver()
# 1.1 Accedo all'indirizzo web da cui voglio estrarre le informazioni
driver.get('https://www.miodottore.it/patologie/ansia/domande')
parole_chiave = ["www.", "http://", "https://", "Dr." ,"Dott.", "Dott.ssa", "Cordiali", "Saluti", "Cordialmente", "Prego", "Arrivederci", "Prof."]
data = []
# 2. Loop per navigare tra le pagine
for i in range(20):
    # 3. Estrazione URL per le domande specificando l'xpath di riferimento sfruttando la libreria By



    for urls in driver.find_elements(By.XPATH, "//div[contains(@class, 'doctor-question-body')]/a[contains(@class, 'text-body')]"):
        #3.1. Estraggo l'attributo per navigare tra le pagine che corrisponde a href
        url = urls.get_attribute("href")
        print("URL QA: {}".format(url))
        print()

        curr_driver = web_driver()
        curr_driver.get(url)

        # 4. Apertura URL domande e estrazione dati:
        WebDriverWait(curr_driver, 20).until(
            EC.presence_of_element_located((By.XPATH, "//main"))
        )


        #5. Extracting and Storing Data:
        #5.1 Salvo la domanda
        try:
            question = curr_driver.find_element(By.XPATH, "//div[@class='h4 font-weight-normal doctor-question-body mb-2']").text
            question_senza_acapo = question.replace('\n', ' ')
            # Rimuovi eventuali spazi extra testo_senza_acapo = ' '
            question_senza_acapo = ' '.join(answer_senza_acapo.split())
        except:
            question = None
            question_senza_acapo = None
        #5.3 Salvo la risposta
        try:
            answer = curr_driver.find_element(By.XPATH, "//div[contains(@class, 'doctor-answer-content') and contains(@class, 'mb-1')]/div" ).text
            stringa_pulita = pulisci_stringa_con_vettore(answer, parole_chiave)
            # Sostituisci gli a capo con uno spazio
            answer_senza_acapo = stringa_pulita.replace('\n', ' ')
            # Rimuovi eventuali spazi extra testo_senza_acapo = ' '
            answer_senza_acapo = ' '.join(answer_senza_acapo.split())
        except:
            answer = None
            answer_senza_acapo = None

        try:
            dottore = curr_driver.find_element(By.XPATH, '//span[@itemprop="name"]').text
        except:
            dottore = None

        try:
            specializzazione = curr_driver.find_element(By.XPATH, "//div[contains(@class, 'small text-muted')]" ).text
        except:
            specializzazione = None

        try:
            Location = curr_driver.find_element(By.XPATH, "//div[contains(@class, 'small text-placeholder')]" ).text
        except:
            Location = None

        question_date = None
        answer_date = None
        # Rimuovi i titoli "Dott.ssa", "Dott." e "Dr." dal nome
        dottore = dottore.replace('Dott.ssa ', '').replace('Dott. ', '').replace('Dott. ', '').replace('Dr. ', '')

        print("Question: {}".format(question_senza_acapo))
        print("Answer: {}".format(answer_senza_acapo))
        print("Doctor: {}".format(dottore))
        print("Specialization: {}".format(specializzazione))
        print("Location: {}".format(Location))
        print()

        data.append({
            "URL": url,
            "Question Date" : question_date,
            "Question": question_senza_acapo,
            "Answer Date": answer_date,
            "Answer": answer_senza_acapo,
            "Doctor": dottore,
            "Specialization": specializzazione,
            "Location": Location
        })

        # 6. Closing WebDriver Instances:
        curr_driver.quit()


    # Trova l'elemento "Avanti" e cliccaci sopra
    button = driver.find_element(By.XPATH, "//li[contains(@class, 'page-item')]/a[contains(@class, 'page-link') and contains(@class, 'page-link-next') and contains(@class, 'd-flex') and contains(@class, 'align-items-center') and contains(@class, 'justify-content-center')]")
    driver.execute_script("arguments[0].click();", button)


    print("Navigating to Next Page")
    print("-----------------------")
    print("-----------------------")
    print()

# 6. Closing WebDriver Instances:
driver.quit()

# Salvo il tutto su df=dataframe, struttura dati tipica di pandas
df = pd.DataFrame(data)

df.head(5)

df.to_csv("/content/drive/MyDrive/miodottore_dataset_ansia.csv", index=False)

"""##Domande sulla Depressione"""

import pandas as pd
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# 1. Inizializzazione e Apertura dell'URL di Destinazione:
driver = web_driver()
# 1.1 Accedo all'indirizzo web da cui voglio estrarre le informazioni
driver.get('https://www.miodottore.it/patologie/depressione/domande')
parole_chiave = ["www.", "http://", "https://", "Dr." ,"Dott.", "Dott.ssa", "Cordiali", "Saluti", "Cordialmente", "Prego", "Arrivederci", "Prof."]
data = []
# 2. Loop per navigare tra le pagine
for i in range(30):
    # 3. Estrazione URL per le domande specificando l'xpath di riferimento sfruttando la libreria By



    for urls in driver.find_elements(By.XPATH, "//div[contains(@class, 'doctor-question-body')]/a[contains(@class, 'text-body')]"):
        #3.1. Estraggo l'attributo per navigare tra le pagine che corrisponde a href
        url = urls.get_attribute("href")
        print("URL QA: {}".format(url))
        print()

        curr_driver = web_driver()
        curr_driver.get(url)

        # 4. Apertura URL domande e estrazione dati:
        WebDriverWait(curr_driver, 20).until(
            EC.presence_of_element_located((By.XPATH, "//main"))
        )


        #5. Extracting and Storing Data:
        #5.1 Salvo la domanda
        try:
            question = curr_driver.find_element(By.XPATH, "//div[@class='h4 font-weight-normal doctor-question-body mb-2']").text
            # Sostituisci gli a capo con uno spazio
            question_senza_acapo = question.replace('\n', ' ')
            # Rimuovi eventuali spazi extra testo_senza_acapo = ' '
            question_senza_acapo = ' '.join(answer_senza_acapo.split())
        except:
            question = None
            question_senza_acapo = None
        #5.3 Salvo la risposta
        try:
            answer = curr_driver.find_element(By.XPATH, "//div[contains(@class, 'doctor-answer-content') and contains(@class, 'mb-1')]/div" ).text
            stringa_pulita = pulisci_stringa_con_vettore(answer, parole_chiave)
            # Sostituisci gli a capo con uno spazio
            answer_senza_acapo = stringa_pulita.replace('\n', ' ')
            # Rimuovi eventuali spazi extra testo_senza_acapo = ' '
            answer_senza_acapo = ' '.join(answer_senza_acapo.split())
        except:
            answer = None
            answer_senza_acapo = None

        try:
            dottore = curr_driver.find_element(By.XPATH, '//span[@itemprop="name"]').text
        except:
            dottore = None

        try:
            specializzazione = curr_driver.find_element(By.XPATH, "//div[contains(@class, 'small text-muted')]" ).text
        except:
            specializzazione = None

        try:
            Location = curr_driver.find_element(By.XPATH, "//div[contains(@class, 'small text-placeholder')]" ).text
        except:
            Location = None

        question_date = None
        answer_date = None
        # Rimuovi i titoli "Dott.ssa", "Dott." e "Dr." dal nome
        dottore = dottore.replace('Dott.ssa ', '').replace('Dott. ', '').replace('Dott. ', '').replace('Dr. ', '')

        print("Question: {}".format(question_senza_acapo))
        print("Answer: {}".format(answer_senza_acapo))
        print("Doctor: {}".format(dottore))
        print("Specialization: {}".format(specializzazione))
        print("Location: {}".format(Location))
        print()

        data.append({
            "URL": url,
            "Question Date" : question_date,
            "Question": question_senza_acapo,
            "Answer Date": answer_date,
            "Answer": answer_senza_acapo,
            "Doctor": dottore,
            "Specialization": specializzazione,
            "Location": Location
        })

        # 6. Closing WebDriver Instances:
        curr_driver.quit()


    # Trova l'elemento "Avanti" e cliccaci sopra
    button = driver.find_element(By.XPATH, "//li[contains(@class, 'page-item')]/a[contains(@class, 'page-link') and contains(@class, 'page-link-next') and contains(@class, 'd-flex') and contains(@class, 'align-items-center') and contains(@class, 'justify-content-center')]")
    driver.execute_script("arguments[0].click();", button)


    print("Navigating to Next Page")
    print("-----------------------")
    print("-----------------------")
    print()

# 6. Closing WebDriver Instances:
driver.quit()

# Salvo il tutto su df=dataframe, struttura dati tipica di pandas
df = pd.DataFrame(data)

df.head(5)

df.to_csv("/content/drive/MyDrive/miodottore_dataset_depressione.csv", index=False)

"""#Scraping sito MEDICITALIA

*A causa di alcuni problemi col reperimento dei dati è stato modificato il codice rispetto ai casi di scraping precedenti, si è sfruttato un driver singolo, poiché il doppio driver dava problemi*

## Domande sulla Cefalea / Colon Irritabile / Alimentazione
"""

def pulisci_stringa_con_vettore(stringa, parole_chiave):
    # Trova l'indice della prima parola chiave trovata nella stringa
    min_indice = len(stringa)
    for parola_chiave in parole_chiave:
        indice = stringa.find(parola_chiave)
        if indice != -1 and indice < min_indice:
            min_indice = indice
    # Se una parola chiave è trovata, ritorna la parte della stringa prima della parola chiave
    if min_indice != len(stringa):
        return stringa[:min_indice]
    # Se nessuna parola chiave è trovata, ritorna la stringa originale
    return stringa

import re

def rimuovi_data_ora(testo):
    # Definisci il pattern regex per la data e l'ora
    pattern = r'\d{2}\.\d{2}\.\d{4} \d{2}:\d{2}'

    # Sostituisci il pattern con una stringa vuota
    testo_pulito = re.sub(pattern, '', testo)

    # Rimuovi eventuali spazi extra
    testo_pulito = testo_pulito.strip()

    return testo_pulito

# Esempio di utilizzo
testo_originale = "Questo è un esempio di testo con una data 25.01.2019 08:50 inclusa."
testo_pulito = rimuovi_data_ora(testo_originale)
print(testo_pulito)  # Output: "Questo è un esempio di testo con una data inclusa."

import pandas as pd
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from datetime import datetime

#https://www.medicitalia.it/consulti/?pagina=2&tag=colon-irritabile
#https://www.medicitalia.it/consulti/?pagina={i}&tag=cefalea
#https://www.medicitalia.it/consulti/?pagina={i}&tag=alimentazione
driver = web_driver()
urls = []
data = []
answer_date = None
Location = None
#Una pool di parole per ripulire la stringa della risposta
parole_chiave = ["www.", "http://", "https://", "Dr." ,"Dott.", "Dott.ssa", "Cordiali", "Saluti", "Cordialmente", "Prego", "Arrivederci", "Prof."]

for i in range(1,60):
    URL = f'https://www.medicitalia.it/consulti/?pagina={i}&tag=alimentazione'
    print(URL)
    driver.get(URL)
    wait = WebDriverWait(driver, 40)

    try:
        # Attempt to click the accept all button, with a timeout of 10 seconds
        wait.until(EC.element_to_be_clickable((By.ID,'pt-accept-all'))).click()
    except TimeoutException:
        # If the button is not found within the timeout, print a message and continue.
        print("Accept all button not found, continuing...")

    links = wait.until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR,"a.titconsulto[href]")))
    urls = []
    for link in links:
        urls.append(link.get_attribute('href'))
    for url in urls:
        driver.get(url)
        wait.until(EC.presence_of_element_located((By.XPATH,'//*[@id="question"]')))

        try:
            question = driver.find_element(By.XPATH, '//div[@class="col cons px-4 pb-0"]').text
            question_pulita = rimuovi_data_ora(question)
            # Sostituisci gli a capo con uno spazio
            question_senza_acapo = question_pulita.replace('\n', ' ')
            # Rimuovi eventuali spazi extra testo_senza_acapo = ' '
            question_senza_acapo = ' '.join(question_senza_acapo.split())

        except:
            question = None
            question_pulita = None
            testo_senza_acapo = None

        try:
            answer = driver.find_element(By.XPATH, "//div[contains(@class, 'col cons px-4 pt-4 pb-0')]").text
            stringa_pulita = pulisci_stringa_con_vettore(answer, parole_chiave)
            # Sostituisci gli a capo con uno spazio
            answer_senza_acapo = stringa_pulita.replace('\n', ' ')
            # Rimuovi eventuali spazi extra testo_senza_acapo = ' '
            answer_senza_acapo = ' '.join(answer_senza_acapo.split())

        except:
            answer = None
            stringa_pulita = None
            answer_senza_acapo = None

        try:
            dottore = driver.find_element(By.XPATH, "//span[@class='d-block text-dark-blue nomespec']").text
            if dottore is not None:
              # Rimuovi i titoli "Dott.ssa", "Dott." e "Dr." dal nome
              dottore = dottore.replace('Dott.ssa ', '').replace('Dott. ', '').replace('Dott. ', '').replace('Dr. ', '')
        except:
            dottore = None

        try:
            Specialization = driver.find_element(By.XPATH, "//span[@class='spec-spec d-block text-dark-blue']").text
        except:
            Specialization = None

        try:
            question_date = driver.find_element(By.XPATH, '//time[@class="text-secondary small"]').text
            if question_date is not None:
              # Converti la data in un oggetto datetime
              data_datetime = datetime.strptime(question_date, "%d.%m.%Y %H:%M")
              # Rimuovi l'ora e mantieni solo la data
              data_solo_data = data_datetime.date()
              # Converti la data in un formato ISO 8601 (senza ora) data_iso
              data_iso = data_solo_data.isoformat()
        except:
            question_date = None
            data_iso = None


        print("Question: {}".format(question_senza_acapo))
        print("Question Date: {}".format(data_iso))
        print("Answer: {}".format(answer_senza_acapo))
        print("Doctor: {}".format(dottore))
        print("Specialization: {}".format(Specialization))
        print("Location: {}".format(Location))
        print(i)

        data.append({
            "URL": url,
            "Question Date" : data_iso,
            "Question": question_senza_acapo,
            "Answer Date": answer_date,
            "Answer": answer_senza_acapo,
            "Doctor": dottore,
            "Specialization": Specialization,
            "Location": Location
        })

#driver.find_element(By.CSS_SELECTOR, ".page-numbers > .icon-chevron-right").click()


driver.quit()

# Create DataFrame
df = pd.DataFrame(data)

df.head(5)

#df.to_csv("/content/drive/MyDrive/medicinaitalia_dataset_cefalea.csv", index=False)
#df.to_csv("/content/drive/MyDrive/medicinaitalia_dataset_colon.csv", index=False)
df.to_csv("/content/drive/MyDrive/medicinaitalia_dataset_alimentazione.csv", index=False)

"""# Merge e Pre-Processing dei dati

##MERGE
"""

import pandas as pd
import os

# Lista per contenere i DataFrame
dataframe = []

# Directory dei file CSV
csv_directory = "/content/drive/MyDrive/"


# Specifica i file CSV con percorsi corretti
csv_files = [
    #Dati dal sito Dica33
    os.path.join(csv_directory, "dica33_dataset_mente.csv"),
    os.path.join(csv_directory, "dica33_dataset_cuore.csv"),
    os.path.join(csv_directory, "dica33_dataset_fegato.csv"),
    os.path.join(csv_directory, "dica33_dataset_mente.csv"),
    os.path.join(csv_directory, "dica33_dataset_naso.csv"),
    #Dati dal sito medicinaitalia
    os.path.join(csv_directory, "medicinaitalia_dataset_alimentazione.csv"),
    os.path.join(csv_directory, "medicinaitalia_dataset_cefalea.csv"),
    os.path.join(csv_directory, "medicinaitalia_dataset_colon.csv"),
    #Dati dal sito miodottore
    os.path.join(csv_directory, "miodottore_dataset_ansia.csv"),
    os.path.join(csv_directory, "miodottore_dataset_depressione.csv"),
]


# Leggi tutti i CSV e aggiungili alla lista
for file in csv_files:
    df = pd.read_csv(file)
    dataframe.append(df)

# Combina tutti i DataFrame in uno solo
merge_df = pd.concat(dataframe, ignore_index=True)

# Salva il file combinato
output_path = os.path.join(csv_directory, "combined_file.csv")
merge_df.to_csv(output_path, index=False)

print(f"Tutti i CSV sono stati combinati e salvati in: {output_path}")

merge_df

# Salva il file combinato su Google Drive
output_path = "/content/drive/My Drive/dataset_completo.csv"
merge_df.to_csv(output_path, index=False)

print(f"Tutti i CSV sono stati combinati e salvati in: {output_path}")

"""## Pre-processing finale per i valori nulli sul Question/Answer"""

# Verifica i valori nulli nelle colonne "Question" e "Answer"
null_values_question = merge_df['Question'].isnull().sum()
null_values_answer = merge_df['Answer'].isnull().sum()

# Stampa i risultati
print(f"Valori nulli nella colonna 'Question': {null_values_question}")
print(f"Valori nulli nella colonna 'Answer': {null_values_answer}")

# Rimuovi le righe dove la colonna 'Question' o 'Answer' contiene valori nulli
df_new = merge_df.dropna(subset=['Question', 'Answer'])

# check ulteriore
null_values_question = df_new['Question'].isnull().sum()
null_values_answer = df_new['Answer'].isnull().sum()

# Stampa i risultati
print(f"Valori nulli nella colonna 'Question': {null_values_question}")
print(f"Valori nulli nella colonna 'Answer': {null_values_answer}")

df_new

import re
# Funzione di normalizzazione che rimuove spazi e tutto ciò che inizia con una parentesi tonda
def normalize_city(city):
    # Controlla se la città è una stringa
    if isinstance(city, str):
        # Rimuove gli spazi bianchi e tutto il testo a partire dalla prima parentesi tonda
        city_cleaned = re.sub(r"\s*\([^)]*\)", "", city).strip()

        # Converte tutto in minuscolo e restituisce la città normalizzata
        return city_cleaned.lower()
    return city

# Normalizza la colonna 'Città': rimuove spazi bianchi e ciò che segue una parentesi tonda
df_new['Location'] = df_new['Location'].apply(normalize_city)

# Visualizza il DataFrame dopo la normalizzazione
print("DataFrame dopo la normalizzazione:")
print(df)

# Salva il DataFrame pulito in un nuovo file CSV
df_new.to_csv('/content/drive/MyDrive/dataset_filtrato.csv', index=False)