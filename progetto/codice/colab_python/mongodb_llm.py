# -*- coding: utf-8 -*-
"""MongoDB_LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EMq23ll5vm4RTzTkM2LT9UH1FZcquE7n

La sequenza di comandi della shell riportata di seguito installa librerie per l'utilizzo di modelli LLM (Large Language Model) open source, modelli di incorporamento e funzionalità di interazione con il database. Queste librerie semplificano lo sviluppo di un sistema RAG, riducendo la complessità a una piccola quantità di codice:

PyMongo: una libreria Python per l'interazione con MongoDB che consente alle funzionalità di connettersi a un cluster e interrogare i dati archiviati in raccolte e documenti.
Pandas: fornisce una struttura di dati per un'elaborazione e un'analisi efficienti dei dati utilizzando Python
Set di dati Hugging Face: contiene set di dati audio, visivi e di testo
Hugging Face Accelerate: astrae la complessità della scrittura di codice che sfrutta acceleratori hardware come le GPU. Accelerate viene sfruttato nell'implementazione per utilizzare il modello Gemma sulle risorse GPU.
Hugging Face Transformers: accesso a una vasta collezione di modelli pre-addestrati
Hugging Face Sentence Transformers: fornisce l'accesso a incorporamenti di frasi, testo e immagini

cleaned_consult_data_filtered.csv
"""

!pip install datasets pandas pymongo sentence_transformers
!pip install -U transformers
# Install below if using GPU
!pip install accelerate
!pip install sentence_transformers

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/dataset_filtrato.csv')
#df = pd.read_csv('/content/drive/MyDrive/miodottore_dataset_ansia.csv')

df

from sentence_transformers import SentenceTransformer
import pandas as pd

# https://huggingface.co/thenlper/gte-large
embedding_model = SentenceTransformer("thenlper/gte-large")


def get_embedding(text: str) -> list[float]:
    if not text.strip():
        print("Attempted to get embedding for empty text.")
        return []

    embedding = embedding_model.encode(text)

    return embedding.tolist()

# Genera embedding per le domande
df['embedding'] = df['Answer'].apply(get_embedding)
df

!pip install -U pymongo

import pymongo
import pandas as pd

def get_mongo_client(mongo_uri):
  """Establish connection to the MongoDB."""
  try:
    client = pymongo.MongoClient(mongo_uri)
    print("Connection to MongoDB successful")
    return client
  except pymongo.errors.ConnectionFailure as e:
    print(f"Connection failed: {e}")
    return None
mongo_uri = "mongodb+srv://antimo:esame123@bigdata.nuu2w.mongodb.net/?retryWrites=true&w=majority&appName=BigData"

mongo_client = get_mongo_client(mongo_uri)

# Ingest data into MongoDB
db = mongo_client['Project']
collection = db['Q&A']

# Delete any existing records in the collection
collection.delete_many({})

documents = df.to_dict('records')
collection.insert_many(documents)

print("Data ingestion into MongoDB completed")

# Esegui una query per ottenere tutti i documenti dalla collezione
cursor = collection.find({})

# Converti i risultati della query in una lista di dizionari
data_list = list(cursor)

# Chiudi il cursore
cursor.close()

# Converti la lista di dizionari in un DataFrame
df = pd.DataFrame(data_list)

#Stampa dataset
df

pipeline_get_date_range = [
    {
        "$match": {
            "Question Date": {"$ne": None},  # Escludi documenti con Question Date nulli
            "Answer Date": {"$ne": None}     # Escludi documenti con Answer Date nulli
        }
    },
    {
        "$group": {
            "_id": None,
            "min_date": {"$min": "$Question Date"},
            "max_date": {"$max": "$Question Date"}
        }
    }
]

date_range = list(collection.aggregate(pipeline_get_date_range))

# Estrai le date minima e massima dalla risposta
min_date = date_range[0]["min_date"] if date_range else None
max_date = date_range[0]["max_date"] if date_range else None


print(min_date)
print(max_date)

def get_doctors_by_specialization(selected_specialization):
    pipeline_doctors = [
        {
            "$match": {
                "Specialization": {"$regex": selected_specialization, "$options": "i"},
                "Location": {"$type": "string"},
                "Doctor": {"$type": "string"}
            }
        },
        {
            "$project": {
                "_id": 0,  # Escludi l'id
                "Location": 1,
                "Doctor": 1,
                "Specialization": 1
            }
        }
    ]
    doctors = list(collection.aggregate(pipeline_doctors))
    return doctors

pipeline_specializations = [
    {
        "$match": {
            "Specialization": {"$type": "string"}  # Considera solo documenti con Specialization valida
        }
    },
    {
        "$group": {
            "_id": "$Specialization"  # Raggruppa per specializzazione unica
        }
    },
    {
        "$sort": {"_id": 1}  # Ordina le specializzazioni in ordine alfabetico
    }
]
specializations = list(collection.aggregate(pipeline_specializations))
specialization_list = [doc['_id'] for doc in specializations]


get_doctors_by_specialization("Oncologo")

"""#LLM"""

def vector_search(user_query, collection):
    """
    Perform a vector search in the MongoDB collection based on the user query.

    Args:
    user_query (str): The user's query string.
    collection (MongoCollection): The MongoDB collection to search.

    Returns:
    list: A list of matching documents.
    """

    # Generate embedding for the user query
    query_embedding = get_embedding(user_query)
    print("Embedding Query:", query_embedding)

    if query_embedding is None or not isinstance(query_embedding, list):
        print("Invalid query or embedding generation failed.")
        return []

    # Define the vector search pipeline
    pipeline = [
        {
            "$vectorSearch": {
                "index": "vector_index",  # The index name
                "path": "embedding",  # The field where embeddings are stored
                "queryVector": query_embedding,  # The vector for the query
                "numCandidates": 150,  # Number of candidate matches to consider
                "limit": 10,  # Return top 4 matches
            }
        },
        {
            "$project": {
                "_id": 0,  # Exclude the _id field
                "Answer": 1,  # Include the Answer field
                "Doctor": 1,  # Include the Doctor field
                "Specialization": 1,  # Include the Specialization field
                "URL": 1,  # Include the URL field
                "score": {"$meta": "vectorSearchScore"}  # Include the search score
            }
        }
    ]

    # Execute the search and check the result
    results = collection.aggregate(pipeline)
    array_of_results = []
    for doc in results:
        array_of_results.append(doc)
    return array_of_results

def get_search_result(query, collection):

    get_knowledge = vector_search(query, collection)
    print(get_knowledge)
    search_result = ""
    for result in get_knowledge:
        answer = result.get('Answer', 'N/A')


    return search_result

user_query = "Dammi informazioni sul diabete"
result = vector_search(user_query, collection)
print(result)

query = "buonasera, assumo benzodiazepine da più di 45 anni a seguito incidente"
result = collection.aggregate([
    {
        "$vectorSearch": {
        "index": "vector_index",
        "path": "embedding",
        "queryVector": get_embedding(query),
        "numCandidates": 5,
        "limit": 5,
        }
        }
]);

for document in result:
    print(f'Nome Dottore: {document["Doctor"]}\n')
    print(f'Specializzazione: {document["Specialization"]}\n')
    print(f'Answer: {document["Answer"]}\n')
    print(f'URL: {document["URL"]}\n')
    print("----------")

user_query = "Sintomi del diabete"
result = vector_search(user_query, collection)
print(result)

from huggingface_hub import notebook_login
notebook_login()

import os
from huggingface_hub import InferenceClient

# Specify search query, retrieve relevant documents, and convert to string
query = "Vietnam"
context_docs = vector_search(query, collection)
answer1 = []
answer2 = []
# Construct prompt for the LLM using the retrieved documents as the context
prompt = f"""
    [INST]
    You are a professional doctor with expertise in anxiety disorders.
    Use the following context to provide a clear, accurate, and concise answer to the question.
    Context: {context_docs}
    Question: {query}
    Answer:
    - Respond only with information from the {context_docs}. Do not use any external knowledge or make assumptions.
    - If there is no relevant information in the {context_docs}, state: "Non ho informazioni a riguardo."
    - Do not add the list of doctors if there is no relevant information in the context.
    - Do not include the list of doctors if no information is available.
    - Use a precise and medically accurate language.
    - Do not repeat words or sentences verbatim from the context.
    - The answer must be in Italian.
    - Start with ("\nBuongiorno,") followed by a clear, well-organized response.
    - End with ("Cordiali Saluti.")

[/INST]

"""

# Authenticate to Hugging Face and access the model
os.environ["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"] = "<token>"
llm = InferenceClient(
    "mistralai/Mixtral-8x7B-Instruct-v0.1",
    token = os.getenv("HF_TOKEN"))
# Prompt the LLM
output = llm.chat_completion(
    messages=[
        {"role": "user", "content": prompt}
        ],
    max_tokens=1000,
    temperature=0.4,  # Risposta meno creativa e più precisa (>1 diventa più creativo ma anche più incerto)
    top_p=0.9,  # Limita la diversità
)
response = output.choices[0].message.content
print(response)

evaluation_prompt = f"""
Valuta la seguente risposta in base a:
1. Pertinenza con il contesto
2. Completezza
3. Coerenza
4. Leggibilità

Risposta: {response}

Punteggio (da 1 a 10):
"""

# Usa il modello GPT-4 per la valutazione
output = llm.chat_completion(messages=[{"role": "user", "content": evaluation_prompt}])
print(output.choices[0].message.content)